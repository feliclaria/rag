{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = \"llama2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sure, here's one:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "Because they make up everything!\n",
      "\n",
      "I hope you found that amusing! Do you want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "model = OllamaLLM(model=MODEL)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)\n",
    "\n",
    "response = model.invoke(\"Tell me a joke\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sure, here's one:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "Because they make up everything!\n",
      "\n",
      "I hope you found that amusing! Do you want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "chain.invoke(\"Tell me a joke\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer the question based on the context below. If you can\\'t \\nanswer the question, reply \"I don\\'t know\".\\n\\nContext: Here is some context\\n\\nQuestion: Here is a question\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Here is some context\", question=\"Here is a question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Sure! Based on the context you provided, my answer would be:\\n\\n\"My name is Santiago.\"'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"context\": \"My parents named me Santiago\", \"question\": \"What's your name'?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test.pdf', 'page': 0}, page_content='Estrategias de RAG y evaluaci´ on de preguntas en modelos de\\nlenguaje\\nFelipe Clari´ a, Santiago Fada\\nFAMaFyC, UNC, Argentina\\nResumen\\nEl uso de Retrieval Augmented Generation (RAG) ha ganado popularidad como una t´ ecnica\\npara mejorar la generaci´ on de respuestas en modelos de lenguaje natural. Esta metodolog´ ıa permite\\nintegrar informaci´ on externa al modelo con el objetivo de proporcionar contexto a las respuestas ge-\\nneradas. En este estudio, exploramos c´ omo RAG puede optimizar la respuesta a preguntas complejas\\ny especializadas, mejorando la calidad de las interacciones con los modelos de lenguaje.\\nIntroducci´ on\\nLos Large Language Model (LLM) han alcanzado un ´ exito notable en diversas tareas de procesamiento\\nde texto, pero todav´ ıa enfrentan limitaciones en tareas espec´ ıficas de dominio o que requieren un alto\\nnivel de conocimiento [1]. Estos modelos son propensos a producir “alucinaciones” cuando se les presentan\\nconsultas que est´ an m´ as all´ a de sus datos de entrenamiento o que exigen informaci´ on actualizada.\\nEn este contexto, el enfoque de Retrieval Augmented Generation (RAG) se presenta como una solu-\\nci´ on prometedora. Al integrar la recuperaci´ on de informaci´ on con la generaci´ on de texto, RAG tiene el\\npotencial de proporcionar respuestas m´ as precisas y contextualizadas, superando algunas de las limita-\\nciones inherentes a los modelos de lenguaje tradicionales [2].\\nEste proyecto se centra en investigar las ventajas de RAG, clasificar los tipos de preguntas que son m´ as\\nadecuadas para este enfoque y analizar la efectividad de estas estrategias en comparaci´ on con modelos\\nque no las implementan.\\nObjetivos\\nEl proyecto busca explorar y analizar los contextos en los que la utilizaci´ on de RAG ofrece ventajas\\nsignificativas sobre los modelos de lenguaje tradicionales (LLM). El objetivo principal es identificar los\\ntipos de preguntas que se responden de manera m´ as efectiva con este enfoque. Evaluaremos c´ omo el uso\\nde RAG puede mejorar la precisi´ on, relevancia y calidad de las respuestas del modelo, en comparaci´ on\\ncon LLMs que no implementen estas estrategias.\\nHip´ otesis\\nProponemos que el uso de RAG proporcionar´ a respuestas m´ as precisas y contextualmente relevantes\\nque un LLM tradicional en la mayor´ ıa de los casos, y creemos que esto ser´ a especialmente evidente con\\npreguntas que requieren informaci´ on actualizada o espec´ ıfica de un dominio acotado.\\nMetodolog´ ıa\\nLos experimentos se realizar´ an dentro de un dominio de textos a definir, que abarcar´ an conocimientos\\nespec´ ıficos sobre ciencias, historia y cultura. Estos dominios deber´ an ser lo suficientemente amplios para\\npermitir la recolecci´ on de datos significativos y facilitar el entrenamiento y la posterior evaluaci´ on de\\nmodelos especializados.\\nSe desarrollar´ a un conjunto de preguntas con la intenci´ on de evaluar la generaci´ on de respuestas. Las\\npreguntas se clasificar´ an en las siguientes categor´ ıas:\\n1'),\n",
       " Document(metadata={'source': 'test.pdf', 'page': 1}, page_content='Preguntas de contexto cerrado Resueltas dentro del conocimiento del modelo.\\nPreguntas contextuales Requieren informaci´ on adicional no contenida en el modelo.\\nPreguntas ambiguas o complejas No pueden ser resueltas adecuadamente con o sin informaci´ on\\nadicional debido a la falta de datos precisos.\\nPreguntas especializadas Implican la necesidad de informaci´ on filtrada o criterios espec´ ıficos.\\nPlanificaci´ on futura\\nSemana 1: Revisi´ on de literatura sobre RAG y definici´ on de contextos espec´ ıficos. Especificaci´ on\\ny formulaci´ on de preguntas.\\nSemana 2: Extracci´ on y preparaci´ on de datos de fuentes relevantes. Implementaci´ on de t´ ecnicas\\nde preprocesamiento y limpieza de datos.\\nSemana 3: Desarrollo y entrenamiento de modelos utilizando los datos extra´ ıdos. Evaluaci´ on inicial\\nde la calidad de las respuestas generadas mediante preguntas de contexto cerrado y contextuales.\\nSemana 4: Experimentaci´ on con preguntas ambiguas y especializadas, analizando la efectividad\\nde RAG en estos casos.\\nSemana 5: Elaboraci´ on del informe final con las comparaciones realizadas y las conclusiones.\\nReferencias\\n[1] Nikhil Kandpal et al. Large Language Models Struggle to Learn Long-Tail Knowledge . 2023. arXiv:\\n2211.08411 [cs.CL] .url:https://arxiv.org/abs/2211.08411 .\\n[2] Yunfan Gao et al. Retrieval-Augmented Generation for Large Language Models: A Survey . 2024.\\narXiv: 2312.10997 [cs.CL] .url:https://arxiv.org/abs/2312.10997 .\\n2')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"test.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test.pdf', 'page': 1}, page_content='Preguntas de contexto cerrado Resueltas dentro del conocimiento del modelo.\\nPreguntas contextuales Requieren informaci´ on adicional no contenida en el modelo.\\nPreguntas ambiguas o complejas No pueden ser resueltas adecuadamente con o sin informaci´ on\\nadicional debido a la falta de datos precisos.\\nPreguntas especializadas Implican la necesidad de informaci´ on filtrada o criterios espec´ ıficos.\\nPlanificaci´ on futura\\nSemana 1: Revisi´ on de literatura sobre RAG y definici´ on de contextos espec´ ıficos. Especificaci´ on\\ny formulaci´ on de preguntas.\\nSemana 2: Extracci´ on y preparaci´ on de datos de fuentes relevantes. Implementaci´ on de t´ ecnicas\\nde preprocesamiento y limpieza de datos.\\nSemana 3: Desarrollo y entrenamiento de modelos utilizando los datos extra´ ıdos. Evaluaci´ on inicial\\nde la calidad de las respuestas generadas mediante preguntas de contexto cerrado y contextuales.\\nSemana 4: Experimentaci´ on con preguntas ambiguas y especializadas, analizando la efectividad\\nde RAG en estos casos.\\nSemana 5: Elaboraci´ on del informe final con las comparaciones realizadas y las conclusiones.\\nReferencias\\n[1] Nikhil Kandpal et al. Large Language Models Struggle to Learn Long-Tail Knowledge . 2023. arXiv:\\n2211.08411 [cs.CL] .url:https://arxiv.org/abs/2211.08411 .\\n[2] Yunfan Gao et al. Retrieval-Augmented Generation for Large Language Models: A Survey . 2024.\\narXiv: 2312.10997 [cs.CL] .url:https://arxiv.org/abs/2312.10997 .\\n2'),\n",
       " Document(metadata={'source': 'test.pdf', 'page': 0}, page_content='Estrategias de RAG y evaluaci´ on de preguntas en modelos de\\nlenguaje\\nFelipe Clari´ a, Santiago Fada\\nFAMaFyC, UNC, Argentina\\nResumen\\nEl uso de Retrieval Augmented Generation (RAG) ha ganado popularidad como una t´ ecnica\\npara mejorar la generaci´ on de respuestas en modelos de lenguaje natural. Esta metodolog´ ıa permite\\nintegrar informaci´ on externa al modelo con el objetivo de proporcionar contexto a las respuestas ge-\\nneradas. En este estudio, exploramos c´ omo RAG puede optimizar la respuesta a preguntas complejas\\ny especializadas, mejorando la calidad de las interacciones con los modelos de lenguaje.\\nIntroducci´ on\\nLos Large Language Model (LLM) han alcanzado un ´ exito notable en diversas tareas de procesamiento\\nde texto, pero todav´ ıa enfrentan limitaciones en tareas espec´ ıficas de dominio o que requieren un alto\\nnivel de conocimiento [1]. Estos modelos son propensos a producir “alucinaciones” cuando se les presentan\\nconsultas que est´ an m´ as all´ a de sus datos de entrenamiento o que exigen informaci´ on actualizada.\\nEn este contexto, el enfoque de Retrieval Augmented Generation (RAG) se presenta como una solu-\\nci´ on prometedora. Al integrar la recuperaci´ on de informaci´ on con la generaci´ on de texto, RAG tiene el\\npotencial de proporcionar respuestas m´ as precisas y contextualizadas, superando algunas de las limita-\\nciones inherentes a los modelos de lenguaje tradicionales [2].\\nEste proyecto se centra en investigar las ventajas de RAG, clasificar los tipos de preguntas que son m´ as\\nadecuadas para este enfoque y analizar la efectividad de estas estrategias en comparaci´ on con modelos\\nque no las implementan.\\nObjetivos\\nEl proyecto busca explorar y analizar los contextos en los que la utilizaci´ on de RAG ofrece ventajas\\nsignificativas sobre los modelos de lenguaje tradicionales (LLM). El objetivo principal es identificar los\\ntipos de preguntas que se responden de manera m´ as efectiva con este enfoque. Evaluaremos c´ omo el uso\\nde RAG puede mejorar la precisi´ on, relevancia y calidad de las respuestas del modelo, en comparaci´ on\\ncon LLMs que no implementen estas estrategias.\\nHip´ otesis\\nProponemos que el uso de RAG proporcionar´ a respuestas m´ as precisas y contextualmente relevantes\\nque un LLM tradicional en la mayor´ ıa de los casos, y creemos que esto ser´ a especialmente evidente con\\npreguntas que requieren informaci´ on actualizada o espec´ ıfica de un dominio acotado.\\nMetodolog´ ıa\\nLos experimentos se realizar´ an dentro de un dominio de textos a definir, que abarcar´ an conocimientos\\nespec´ ıficos sobre ciencias, historia y cultura. Estos dominios deber´ an ser lo suficientemente amplios para\\npermitir la recolecci´ on de datos significativos y facilitar el entrenamiento y la posterior evaluaci´ on de\\nmodelos especializados.\\nSe desarrollar´ a un conjunto de preguntas con la intenci´ on de evaluar la generaci´ on de respuestas. Las\\npreguntas se clasificar´ an en las siguientes categor´ ıas:\\n1')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"semana 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: ¿De qué trata el documento?\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"¿De qué trata el documento?\",\n",
    "    \"¿Cuál es la hipótesis?\",\n",
    "    \"¿Cuáles son los objetivos de cada semana?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.batch([{\"question\": q} for q in questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in chain.stream({\"question\": \"¿Qué es un sistema de amortización?\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
